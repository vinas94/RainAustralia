{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9bada4-cf40-437b-9b05-b9fd9cdb7dd9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# graphics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# custom functions\n",
    "from custom import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "wandb.login()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653b9a0-0449-4d9e-b6da-9ed964c6a1d7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Defining the model parameters and the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19abde-5497-4ba3-9f96-c3fe9f3b6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'batch_size': 1024,\n",
    "    'n_days': 7,\n",
    "    'epochs': 500,\n",
    "    'lr': 0.01,\n",
    "    'cities': [],\n",
    "    'features': [],\n",
    "    'oversample': True,\n",
    "    'scheduler': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b86ca4-a242-484e-adbe-28919ddd7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(PARAMS):\n",
    "\n",
    "    with wandb.init(project='RainAustralia', config=PARAMS):\n",
    "        PARAMS = wandb.config\n",
    "        run_id = wandb.run.id\n",
    "        \n",
    "        # parsing and loading the data\n",
    "        trainloader, testloader = parse_and_prep_data(PARAMS)\n",
    "\n",
    "        # setting some more parameters\n",
    "        r = trainloader.dataset.len / trainloader.dataset.y.sum() - 1\n",
    "        r -= 1.5\n",
    "        r = 1\n",
    "        wandb.config.update({'r': r, 'n_features': trainloader.dataset.x.shape[2]})\n",
    "        PARAMS = wandb.config\n",
    "\n",
    "        # defining the model and other functions\n",
    "        model, loss, optimiser, scheduler = define_model(PARAMS)\n",
    "\n",
    "        # starting the training loop\n",
    "        train(trainloader, testloader, model, loss, optimiser, scheduler)\n",
    "    \n",
    "    return model, run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04330f8c-baa0-4f54-83fb-66f567e0f6f7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Parsing and prepping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-husband",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_and_prep_data(PARAMS):\n",
    "\n",
    "    # Reading in the data\n",
    "    X_train, y_train = read_and_parse('train', cities=PARAMS['cities'], columns=PARAMS['features'])\n",
    "    X_test, y_test = read_and_parse('test', cities=PARAMS['cities'], columns=PARAMS['features'])\n",
    "\n",
    "    # Transforming data into input/output pairs\n",
    "    X_train, y_train = transform_to_TS(X_train, y_train, n=PARAMS['n_days'])\n",
    "    X_test, y_test = transform_to_TS(X_test, y_test, n=PARAMS['n_days'])\n",
    "\n",
    "    # Oversampling minority class\n",
    "    if PARAMS['oversample']:\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        _ = oversample.fit_resample(X_train[:,:,0], y_train)\n",
    "        X_train = X_train[oversample.sample_indices_]\n",
    "        y_train = y_train[oversample.sample_indices_]\n",
    "\n",
    "    # Creating torch class datasets\n",
    "    train_dataset = timeseries(X_train, y_train)\n",
    "    test_dataset = timeseries(X_test, y_test)\n",
    "\n",
    "    # Setting up dataloaders\n",
    "    trainloader = DataLoader(train_dataset, shuffle=True, batch_size=PARAMS['batch_size'])\n",
    "    testloader = DataLoader(test_dataset, shuffle=True, batch_size=PARAMS['batch_size'])\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8587f-2b42-418d-9a9a-eece078f38a2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(PARAMS):\n",
    "\n",
    "    model = LSTM(PARAMS['n_features'])\n",
    "    loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(PARAMS['r'])) # pos_weight > 1 will increase the recall while pos_weight < 1 will increase the precision.\n",
    "    optimiser = optim.Adam(model.parameters(), lr=PARAMS['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, factor=0.1, patience=10, verbose=True)\n",
    "    \n",
    "    return model, loss, optimiser, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850d37b-642f-4233-8fc0-aac600187084",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Defining training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, testloader, model, loss, optimiser, scheduler):\n",
    "\n",
    "    wandb.watch(model, loss, log='all', log_freq=10)\n",
    "    pbar = tqdm(range(PARAMS['epochs']))\n",
    "    for epoch in pbar:\n",
    "\n",
    "        model.train()\n",
    "        loss_tr = 0\n",
    "        corr_tr = 0\n",
    "        for data in trainloader:\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # forward + loss + backward + optimise (update weights)\n",
    "            outputs = model(inputs)\n",
    "            L = loss(outputs, labels)\n",
    "            L.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # keep track of the loss and accuracy this epoch\n",
    "            loss_tr += L.item()\n",
    "            preds = torch.sigmoid(outputs).round()\n",
    "            corr_tr += (preds==labels).sum().item()\n",
    "\n",
    "        if PARAMS['scheduler']:\n",
    "            scheduler.step(loss_tr)\n",
    "            wandb.log({'lr': optimiser.param_groups[0]['lr']}, step=epoch)\n",
    "\n",
    "        acc_tr = corr_tr / trainloader.dataset.x.shape[0]\n",
    "        loss_tr = loss_tr / (trainloader.dataset.x.shape[0] / testloader.dataset.x.shape[0])\n",
    "\n",
    "        # Rescaling to fit test data\n",
    "        k = len(trainloader) / (trainloader.dataset.len / testloader.dataset.len)\n",
    "        loss_tr /= k\n",
    "\n",
    "        wandb.log({'loss_tr': loss_tr, 'acc_tr': acc_tr}, step=epoch)\n",
    "\n",
    "\n",
    "        # Model evaluation\n",
    "        model.eval()\n",
    "\n",
    "        # test accuracy\n",
    "        y_hat_test = get_predictions(testloader.dataset.x, model=model)\n",
    "        acc_tt = accuracy_score(testloader.dataset.y, y_hat_test)\n",
    "\n",
    "        # test loss\n",
    "        outputs = model(testloader.dataset.x)\n",
    "        loss_tt = loss(outputs, testloader.dataset.y).item()\n",
    "\n",
    "        # logging metadata\n",
    "        wandb.log({'loss_tt': loss_tt, 'acc_tt': acc_tt,\n",
    "                   'f1': f1_score(testloader.dataset.y, y_hat_test),\n",
    "                   'f1_avg': np.mean([f1_score(testloader.dataset.y, y_hat_test), f1_score(1-testloader.dataset.y, 1-y_hat_test)])}, step=epoch)\n",
    "\n",
    "\n",
    "        pbar.set_postfix({'test_accuracy': acc_tt,  'test_loss': loss_tt, 'train_accuracy': acc_tr, 'train_loss': loss_tr})\n",
    "        \n",
    "    # storing classification report and confusion matrix\n",
    "    clr = class_report(testloader.dataset.y, y_hat_test, reset_index=True)\n",
    "    wandb.log({'classification_report': wandb.Table(dataframe=clr)})\n",
    "    wandb.log({'confusion_matrix':\n",
    "           wandb.plot.confusion_matrix(y_true=testloader.dataset.y.squeeze().numpy(),\n",
    "                                       preds=y_hat_test.squeeze())})\n",
    "    \n",
    "    # storing the model architecture and weights\n",
    "    torch.onnx.export(model, testloader.__iter__().__next__()[0][0].unsqueeze(0), './model.onnx')\n",
    "    torch.save(model.state_dict(), './model.weights')\n",
    "    wandb.save('./model.onnx')\n",
    "    wandb.save('./model.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8402c7-61e0-465a-9a7f-703c470ce336",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Running the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b939c-c1da-4442-9867-0f49c042e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, run_id = pipeline(PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6d7d0-f791-4968-b34e-4919e6909759",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Loading the trained model and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44310d26-c349-4c1c-a2fa-e891c2065933",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.Api().run(f'/RainAustralia/{run_id}')\n",
    "run.file('model.weights').download(replace=True)\n",
    "model = LSTM(run.config['n_features'])\n",
    "model.load_state_dict(torch.load('./model.weights'))\n",
    "history = run.history()\n",
    "_, testloader = parse_and_prep_data(run.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485994b0-51f3-4c2d-aa79-31838c392503",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Preliminary visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "\n",
    "def reorderLegend(order, ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend([handles[i] for i in order], [labels[i] for i in order])\n",
    "    \n",
    "def left(ax=ax[0]):\n",
    "    ax.plot(history['acc_tt'], label='test_acc', color='#FF7F0E')\n",
    "    ax.plot(history['acc_tr'], label='train_acc', color='#1F77B4')\n",
    "    ax.plot(history['f1_avg'], label='avg_f1_score', color='#2CA02C')\n",
    "    reorderLegend([1, 0, 2], ax)\n",
    "    ax.grid()\n",
    "    ax.set_ylim([0.65, 0.88])\n",
    "    ax.set_title('Accuracy curves')\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "    \n",
    "def mid(ax=ax[1]):\n",
    "    ax.plot(history['loss_tr'], label='train_loss')\n",
    "    ax.plot(history['loss_tt'], label='test_loss')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_title('Loss curves')\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "def right(ax=ax[2]):\n",
    "    plot_conf_matrix(testloader.dataset.y, y_hat_test, normalise='true', ax=ax)\n",
    "    ax.set_title('Confusion matrix, normalised by \\'True\\'')\n",
    "    \n",
    "\n",
    "y_hat_test = get_predictions(testloader.dataset.x, model=model)\n",
    "\n",
    "left()\n",
    "mid()\n",
    "right()\n",
    "\n",
    "# plt.savefig('./Plots/<>.png', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679a5a0-59af-4753-9b09-b3046ae41e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = class_report(testloader.dataset.y, y_hat_test)\n",
    "clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55915c38-6333-4986-bf70-51bdc98d7bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "neptune": {
   "notebookId": "3b18d8ef-8329-4d34-a31f-b73544421fca",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
